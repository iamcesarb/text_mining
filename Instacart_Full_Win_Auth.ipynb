{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pymssql\r\n",
    "import pyodbc\r\n",
    "import pandas as pd\r\n",
    "from datetime import datetime as dt"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\bustossanchez.9\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\compat\\_optional.py:138: UserWarning: Pandas requires version '2.7.0' or newer of 'numexpr' (version '2.6.9' currently installed).\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# SERVER = 'TPCCP-DB09\\SCNEAR'\r\n",
    "#USERNAME = 'ScAdUser'\r\n",
    "#PASSWORD = '5c0r3Card*77'\r\n",
    "conn = pyodbc.connect('Driver={ODBC Driver 17 for SQL Server};'\r\n",
    "                      'Server= TPCCP-DB09\\SCNEAR;'\r\n",
    "                      'Database=Infinite;'\r\n",
    "                      'Trusted_Connection=yes;')\r\n",
    "\r\n",
    "QUERY = '''SELECT [SURVEY_RETURNED_DATE] as 'Date'\r\n",
    "      ,[AGENT_NAME] as 'Agent'\r\n",
    "      ,[EMAIL]\r\n",
    "      , CASE WHEN [TICKET_TYPE_SIMPLIFIED] like 'Instacart Customers' OR [USER_CHANNEL] like 'Customer' THEN 'Customers' ELSE 'Shoppers' END Type\r\n",
    "\t  ,CASE WHEN [CHANNEL_SIMPLIFIED] like 'Instacart Generated' OR [CHANNEL_SIMPLIFIED] like 'Ratings Flow' OR [CONTACT_CHANNEL] like 'Instacart Generated' OR [CONTACT_CHANNEL] like 'Ratings Flow' THEN 'Email'\r\n",
    "\t\t\tWHEN [CHANNEL_SIMPLIFIED] like 'Phone' OR [CONTACT_CHANNEL] like 'Phone' THEN 'Phone'\r\n",
    "\t\t\tWHEN [CHANNEL_SIMPLIFIED] like 'Chat' OR [CONTACT_CHANNEL] like 'Chat' THEN 'Chat'\r\n",
    "\t\t\tELSE 'Email' END Channel  \t  \r\n",
    "\t  ,[SUBCASE_TYPE_LEVEL2] as 'CR'\r\n",
    "      ,CASE WHEN [MENU_PATH_NAME] like 'My Missing Order' OR [MENU_PATH_NAME] like 'Missing Order' OR [MENU_PATH_NAME] like 'Customer Support/Missing Order' OR [MENU_PATH_NAME] like '' OR [MENU_PATH_NAME] is null THEN 'FDO' \r\n",
    "\t  WHEN [MENU_PATH_NAME] like 'Customer Support/Costco Order' OR [MENU_PATH_NAME] like 'Costco Order' OR [MENU_PATH_NAME] like 'Active Costco Order' then 'COSTCO' ELSE 'T1' END Menu_Path\r\n",
    "\t  , CASE WHEN [Rating] like 'good' THEN 'Good' ELSE 'Bad' END CSAT\r\n",
    "\t  ,nullif(RATING_COMMENT,'') as 'Feedback'\r\n",
    "      ,[TOUCH]\r\n",
    "      ,[WAREHOUSE]\r\n",
    "\r\n",
    "\t \r\n",
    "  FROM [Instacart].[dbo].[tbSFTP_CSAT] with(nolock)\r\n",
    "  WHERE ([SURVEY_RETURNED_DATE] >= '2022-01-01') and ([CONTACT_CENTER] like 'Teleperformance - Lima')\r\n",
    "  ORDER BY [Date] DESC'''\r\n",
    "################### DONT TOUCH ###################\r\n",
    "#connection = pymssql.connect( server = SERVER, user = USERNAME, password = PASSWORD, database = 'Instacart')\r\n",
    "\r\n",
    "#cursor = connection.cursor(as_dict=True)\r\n",
    "#cursor.execute(QUERY)\r\n",
    "#data = cursor.fetchall()\r\n",
    "\r\n",
    "BDInfinite = pd.read_sql(sql=QUERY,con=conn)\r\n",
    "\r\n",
    "df=BDInfinite\r\n",
    "df"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Agent</th>\n",
       "      <th>EMAIL</th>\n",
       "      <th>Type</th>\n",
       "      <th>Channel</th>\n",
       "      <th>CR</th>\n",
       "      <th>Menu_Path</th>\n",
       "      <th>CSAT</th>\n",
       "      <th>Feedback</th>\n",
       "      <th>TOUCH</th>\n",
       "      <th>WAREHOUSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-06-21</td>\n",
       "      <td>Silvia Ojeda Vizcarra</td>\n",
       "      <td>silvia.ojedavizcarra@teleperformance.com</td>\n",
       "      <td>Customers</td>\n",
       "      <td>Chat</td>\n",
       "      <td>Entire Delivery Missing</td>\n",
       "      <td>FDO</td>\n",
       "      <td>Bad</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>Best Buy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-06-21</td>\n",
       "      <td>Ingrid Pilar Rodriguez Pastor</td>\n",
       "      <td>ingrid.rodriguezpastor@teleperformance.co</td>\n",
       "      <td>Customers</td>\n",
       "      <td>Chat</td>\n",
       "      <td>Entire Delivery Missing</td>\n",
       "      <td>FDO</td>\n",
       "      <td>Good</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>Safeway</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-06-21</td>\n",
       "      <td>Carlos Alejandro Archila Ruiz</td>\n",
       "      <td>carlos.archilaruiz@teleperformance.com</td>\n",
       "      <td>Customers</td>\n",
       "      <td>Chat</td>\n",
       "      <td>Entire Delivery Missing</td>\n",
       "      <td>FDO</td>\n",
       "      <td>Good</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>GIANT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-06-21</td>\n",
       "      <td>Arnold Gongora Rios</td>\n",
       "      <td>arnold.gongorarios@teleperformance.com</td>\n",
       "      <td>Customers</td>\n",
       "      <td>Chat</td>\n",
       "      <td>Entire Delivery Missing</td>\n",
       "      <td>FDO</td>\n",
       "      <td>Bad</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>Wegmans</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-06-21</td>\n",
       "      <td>Paolo Velarde Jimenez</td>\n",
       "      <td>paolo.velardejimenez@teleperformance.co</td>\n",
       "      <td>Customers</td>\n",
       "      <td>Phone</td>\n",
       "      <td>Not Enough Information</td>\n",
       "      <td>T1</td>\n",
       "      <td>Good</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>789558</th>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>Jairo Alfredo Silva Vilela</td>\n",
       "      <td>jairo.silvavilela@teleperformance.com</td>\n",
       "      <td>Customers</td>\n",
       "      <td>Chat</td>\n",
       "      <td>Entire Delivery Missing</td>\n",
       "      <td>FDO</td>\n",
       "      <td>Good</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>Walgreens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>789559</th>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>Miguel Ramirez Jara</td>\n",
       "      <td>miguel.ramirezjara@teleperformance.com</td>\n",
       "      <td>Customers</td>\n",
       "      <td>Phone</td>\n",
       "      <td>Change Delivery Instructions</td>\n",
       "      <td>T1</td>\n",
       "      <td>Good</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>Food Lion Now</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>789560</th>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>Marcos Jose Marcano Pena</td>\n",
       "      <td>marcos.marcanopena@teleperformance.com</td>\n",
       "      <td>Customers</td>\n",
       "      <td>Phone</td>\n",
       "      <td>Where Is My Order</td>\n",
       "      <td>T1</td>\n",
       "      <td>Good</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>Food Lion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>789561</th>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>Gina Babilon Babilon</td>\n",
       "      <td>gina.babilonbabilon@teleperformance.com</td>\n",
       "      <td>Customers</td>\n",
       "      <td>Chat</td>\n",
       "      <td>Entire Delivery Missing</td>\n",
       "      <td>FDO</td>\n",
       "      <td>Good</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>Shoppers Drug Mart</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>789562</th>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>John Gyber Marquez Argote</td>\n",
       "      <td>john.marquezargote@teleperformance.com</td>\n",
       "      <td>Customers</td>\n",
       "      <td>Chat</td>\n",
       "      <td>Entire Delivery Missing</td>\n",
       "      <td>FDO</td>\n",
       "      <td>Good</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>Publix</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>789563 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Date                          Agent  \\\n",
       "0       2023-06-21          Silvia Ojeda Vizcarra   \n",
       "1       2023-06-21  Ingrid Pilar Rodriguez Pastor   \n",
       "2       2023-06-21  Carlos Alejandro Archila Ruiz   \n",
       "3       2023-06-21            Arnold Gongora Rios   \n",
       "4       2023-06-21          Paolo Velarde Jimenez   \n",
       "...            ...                            ...   \n",
       "789558  2022-01-01     Jairo Alfredo Silva Vilela   \n",
       "789559  2022-01-01            Miguel Ramirez Jara   \n",
       "789560  2022-01-01       Marcos Jose Marcano Pena   \n",
       "789561  2022-01-01           Gina Babilon Babilon   \n",
       "789562  2022-01-01      John Gyber Marquez Argote   \n",
       "\n",
       "                                            EMAIL       Type Channel  \\\n",
       "0        silvia.ojedavizcarra@teleperformance.com  Customers    Chat   \n",
       "1       ingrid.rodriguezpastor@teleperformance.co  Customers    Chat   \n",
       "2          carlos.archilaruiz@teleperformance.com  Customers    Chat   \n",
       "3          arnold.gongorarios@teleperformance.com  Customers    Chat   \n",
       "4         paolo.velardejimenez@teleperformance.co  Customers   Phone   \n",
       "...                                           ...        ...     ...   \n",
       "789558      jairo.silvavilela@teleperformance.com  Customers    Chat   \n",
       "789559     miguel.ramirezjara@teleperformance.com  Customers   Phone   \n",
       "789560     marcos.marcanopena@teleperformance.com  Customers   Phone   \n",
       "789561    gina.babilonbabilon@teleperformance.com  Customers    Chat   \n",
       "789562     john.marquezargote@teleperformance.com  Customers    Chat   \n",
       "\n",
       "                                  CR Menu_Path  CSAT Feedback  TOUCH  \\\n",
       "0            Entire Delivery Missing       FDO   Bad     None      1   \n",
       "1            Entire Delivery Missing       FDO  Good     None      2   \n",
       "2            Entire Delivery Missing       FDO  Good     None      2   \n",
       "3            Entire Delivery Missing       FDO   Bad     None      2   \n",
       "4             Not Enough Information        T1  Good     None      1   \n",
       "...                              ...       ...   ...      ...    ...   \n",
       "789558       Entire Delivery Missing       FDO  Good     None      1   \n",
       "789559  Change Delivery Instructions        T1  Good     None      1   \n",
       "789560             Where Is My Order        T1  Good     None      1   \n",
       "789561       Entire Delivery Missing       FDO  Good     None      1   \n",
       "789562       Entire Delivery Missing       FDO  Good     None      1   \n",
       "\n",
       "                 WAREHOUSE  \n",
       "0                 Best Buy  \n",
       "1                  Safeway  \n",
       "2                    GIANT  \n",
       "3                  Wegmans  \n",
       "4                     None  \n",
       "...                    ...  \n",
       "789558           Walgreens  \n",
       "789559       Food Lion Now  \n",
       "789560           Food Lion  \n",
       "789561  Shoppers Drug Mart  \n",
       "789562              Publix  \n",
       "\n",
       "[789563 rows x 11 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "df[\"len\"]=df[\"Feedback\"].apply(lambda x: len(str(x)))\r\n",
    "df=df[df[\"len\"] >4]\r\n",
    "df=df.drop(\"len\", axis=1)\r\n",
    "df=df.set_index(\"Date\")\r\n",
    "df.to_csv(r\"C:\\Users\\bustossanchez.9\\OneDrive - Teleperformance\\Procesos\\Instacard\\RawData\\Data_TM.csv\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "import nltk\r\n",
    "from nltk.probability import FreqDist\r\n",
    "import pandas as pd\r\n",
    "import datetime as dt\r\n",
    "import numpy as np\r\n",
    "from collections import OrderedDict\r\n",
    "pd.options.mode.chained_assignment = None\r\n",
    "import unicodedata\r\n",
    "import regex as re\r\n",
    "from nltk.stem import WordNetLemmatizer\r\n",
    "from textblob import TextBlob"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:37: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  LARGE_SPARSE_SUPPORTED = LooseVersion(scipy_version) >= '0.14.0'\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\image.py:167: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  dtype=np.int):\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:35: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps,\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:597: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:836: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:862: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, positive=False):\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1097: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1344: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1480: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, positive=False):\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\randomized_l1.py:152: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  precompute=False, eps=np.finfo(np.float).eps,\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\randomized_l1.py:320: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, random_state=None,\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\randomized_l1.py:580: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=4 * np.finfo(np.float).eps, n_jobs=None,\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "#LEER DATA\r\n",
    "cols=[\"Date\", \"Type\", \"Channel\", \"CSAT\", \"Feedback\"]\r\n",
    "df=pd.read_csv(r\"C:\\Users\\bustossanchez.9\\OneDrive - Teleperformance\\Procesos\\Instacard\\RawData\\Data_TM.csv\",usecols=cols)\r\n",
    "#OBTENER EL MES\r\n",
    "df[\"Date\"]=pd.to_datetime(df['Date'])\r\n",
    "df[\"Month\"]=df.Date.dt.month_name()\r\n",
    "df[\"Year\"]=df.Date.dt.year\r\n",
    "df=df.drop(\"Date\", axis=1)\r\n",
    "#Crea columna con el CSAT/lob y borra las otras columnas:\r\n",
    "df['index_2'] = df[[\"CSAT\", \"Channel\", \"Type\"]].apply(lambda x:'/'.join(x), axis = 1)\r\n",
    "df = df.drop([\"CSAT\", \"Channel\", \"Type\"], axis = 1)\r\n",
    "#Crea un index doble, por semana y CSAT/Lob. \r\n",
    "df = df.set_index([\"Year\",\"Month\", \"index_2\"]).sort_index(ascending = [True, True]).astype(str)\r\n",
    "#Agrupa para que solo se tengan por cada semana, 1 fila de CSAT/lob y aplica transform para unir todo el texto que aplica en una celda. \r\n",
    "df= df.groupby([\"Year\",\"Month\", \"index_2\"]).transform(lambda x : ' '.join(x)).drop_duplicates(keep = 'first')\r\n",
    "df[\"Feedback\"] = df[\"Feedback\"].apply(lambda x: str(x).lower())\r\n",
    "df['Feedback'] = df['Feedback'].apply(lambda x: x.encode('utf-8').decode('ascii', 'ignore'))\r\n",
    "df[\"words\"] = df[\"Feedback\"].apply(lambda x: list(nltk.word_tokenize(x)))\r\n",
    "common_words = [\"nbsp\",'<', 'br', '>',\"!\",\".\",\",\", \"...\",\":\", \"n't\",\"(\",\")\",\"wasn't\",\"\", \"nan\"]\r\n",
    "\r\n",
    "#Remover stopwords y puntuacion\r\n",
    "def remove_stopwords(listica):\r\n",
    "    from nltk.corpus import stopwords                   \r\n",
    "    stop_words = set(stopwords.words(\"english\"))        #Getting a list of stopwords\r\n",
    "    removedstopwords = [w for w in listica if not w in stop_words and w not in common_words and len(w) > 2]  # code to remove stop words\r\n",
    "    return removedstopwords\r\n",
    "\r\n",
    "df[\"words\"] = df[\"words\"].apply(lambda x: remove_stopwords(x))\r\n",
    "\r\n",
    "#Lemmatizacion: \r\n",
    "def lemmatizacion(listica2):\r\n",
    "    WNlemma = nltk.WordNetLemmatizer()       #Getting a list of stopwords\r\n",
    "    lemmatized_output = [WNlemma.lemmatize(w, pos = \"v\") for w in listica2]  # code to remove stop words\r\n",
    "    return lemmatized_output\r\n",
    "\r\n",
    "df[\"lemma\"]=df[\"words\"].apply(lambda x: lemmatizacion(x))\r\n",
    "df=df.drop([\"words\", \"Feedback\"], axis=1)\r\n",
    "\r\n",
    "df1=df.copy()\r\n",
    "df2=df.copy()\r\n",
    "df3=df.copy()\r\n",
    "\r\n",
    "#### WORDS ####\r\n",
    "df1[\"frecwords\"]=df1[\"lemma\"].apply(lambda x: FreqDist(x))\r\n",
    "df1.reset_index(inplace=True)\r\n",
    "df1_temp =pd.DataFrame(pd.DataFrame(df1['frecwords'].values.tolist()).stack().reset_index(level=1))\r\n",
    "df1_temp.columns = ['Word', 'Frequency']\r\n",
    "df1 = df1.join(df1_temp)\r\n",
    "df1[[\"CSAT\", \"Channel\", \"Type\"]]=df1['index_2'].str.split('/', 2, expand=True)\r\n",
    "df1.Word=df1.Word.apply(lambda x: x.strip())\r\n",
    "df1.drop([\"frecwords\",\"lemma\",\"index_2\"], axis=1,inplace=True)\r\n",
    "df1=df1[df1[\"Frequency\"]>1]\r\n",
    "df1 = df1[[\"Year\",'Month','CSAT','Channel','Type','Word','Frequency']]\r\n",
    "df1 = df1.rename(columns={'CSAT':'Csat','Frequency':'Frecuency'})\r\n",
    "df1.to_csv(r\"C:\\Users\\bustossanchez.9\\OneDrive - Teleperformance\\Procesos\\Instacard\\DataPBI\\Words.csv\",index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "##### BIGRAMS ####\r\n",
    "\r\n",
    "def common_bigramss(listica3):\r\n",
    "    md_bigrams = list(nltk.bigrams(listica3))\r\n",
    "    threshold = 2\r\n",
    "    filtered_bigrams = [bigram for bigram in md_bigrams if len(bigram[0])>threshold and len(bigram[1])>threshold]\r\n",
    "    filtered_bigram_dist = FreqDist(filtered_bigrams)\r\n",
    "    return filtered_bigram_dist\r\n",
    "\r\n",
    "df2[\"bigrams\"]=df2[\"lemma\"].apply(lambda x: common_bigramss(x))\r\n",
    "df2.reset_index(inplace=True)\r\n",
    "df2_temp =pd.DataFrame(pd.DataFrame(df2['bigrams'].values.tolist()).stack().reset_index(level=1))\r\n",
    "df2_temp.columns = ['Bigram', 'Frequency']\r\n",
    "df2_temp[[\"Word1\",\"Word2\"]]=pd.DataFrame(df2_temp['Bigram'].tolist(), index=df2_temp.index)\r\n",
    "df2 = df2.join(df2_temp)\r\n",
    "df2[[\"CSAT\", \"Channel\", \"Type\"]]=df2['index_2'].str.split('/', 2, expand=True)\r\n",
    "df2.Word1=df2.Word1.apply(lambda x: str(x))\r\n",
    "df2.Word1=df2.Word1.apply(lambda x: x.strip())\r\n",
    "df2.Word2=df2.Word2.apply(lambda x: str(x))\r\n",
    "df2.Word2=df2.Word2.apply(lambda x: x.strip())\r\n",
    "df2.drop([\"bigrams\",\"Bigram\",\"lemma\",\"index_2\"], axis=1,inplace=True)\r\n",
    "df2=df2[df2[\"Frequency\"]>1]\r\n",
    "df2 = df2.rename(columns={'CSAT':'Csat','Frequency':'Frecuency'})\r\n",
    "df2 = df2[[\"Year\",'Month','Csat','Channel','Type','Word1','Word2','Frecuency']]\r\n",
    "df2.head()\r\n",
    "\r\n",
    "df2.to_csv(r\"C:\\Users\\bustossanchez.9\\OneDrive - Teleperformance\\Procesos\\Instacard\\DataPBI\\Bigrams.csv\",index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "###### Lista de Palabras Unicas ################################################\r\n",
    "\r\n",
    "df=pd.read_csv(r\"C:\\Users\\bustossanchez.9\\OneDrive - Teleperformance\\Procesos\\Instacard\\DataPBI\\Words.csv\")\r\n",
    "words=df[\"Word\"]\r\n",
    "unique_words=words.unique()\r\n",
    "key_unique_words = pd.DataFrame(unique_words)\r\n",
    "key_unique_words .rename(columns = {0:'Word'}, inplace = True)\r\n",
    "key_unique_words[\"Word\"]= key_unique_words[\"Word\"].map(str)\r\n",
    "key_unique_words=key_unique_words[\"Word\"].apply(lambda x: x.strip())\r\n",
    "key_unique_words.to_csv(r\"C:\\Users\\bustossanchez.9\\OneDrive - Teleperformance\\Procesos\\Instacard\\DataPBI\\Key_Unique_Words.csv\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "#### TRIGRAMS #####\r\n",
    "\r\n",
    "def common_trigrams(listica3):\r\n",
    "    md_trigrams = list(nltk.trigrams(listica3))\r\n",
    "    filtered_bigram_dist = FreqDist(md_trigrams)\r\n",
    "    return filtered_bigram_dist\r\n",
    "\r\n",
    "df3[\"trigrams\"]=df3[\"lemma\"].apply(lambda x: common_trigrams(x))\r\n",
    "df3.reset_index(inplace=True)\r\n",
    "df3_temp =pd.DataFrame(pd.DataFrame(df3['trigrams'].values.tolist()).stack().reset_index(level=1))\r\n",
    "df3_temp.columns = ['trigram', 'Frequency']\r\n",
    "df3_temp[[\"Word1\",\"Word2\",\"Word3\"]]=pd.DataFrame(df3_temp['trigram'].tolist(), index=df3_temp.index)\r\n",
    "df3 = df3.join(df3_temp)\r\n",
    "df3[[\"CSAT\", \"Channel\", \"Type\"]]=df3['index_2'].str.split('/', 2, expand=True)\r\n",
    "df3.Word1=df3.Word1.apply(lambda x: str(x).strip())\r\n",
    "df3.Word2=df3.Word2.apply(lambda x: str(x).strip())\r\n",
    "df3.Word3=df3.Word3.apply(lambda x: str(x).strip())\r\n",
    "df3.drop([\"trigrams\",\"trigram\",\"lemma\",\"index_2\"], axis=1,inplace=True)\r\n",
    "df3=df3[df3[\"Frequency\"]>1]\r\n",
    "df3 = df3.rename(columns={'CSAT':'Csat','Frequency':'Frecuency'})\r\n",
    "df3 = df3[[\"Year\",'Month','Csat','Channel','Type','Word1','Word2',\"Word3\",'Frecuency']]\r\n",
    "df3.to_csv(r\"C:\\Users\\bustossanchez.9\\OneDrive - Teleperformance\\Procesos\\Instacard\\DataPBI\\Trigrams.csv\",index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "import nltk\r\n",
    "from nltk.probability import FreqDist\r\n",
    "import pandas as pd\r\n",
    "import datetime as dt\r\n",
    "import numpy as np\r\n",
    "from collections import OrderedDict\r\n",
    "pd.options.mode.chained_assignment = None\r\n",
    "import unicodedata\r\n",
    "import regex as re\r\n",
    "from nltk.stem import WordNetLemmatizer\r\n",
    "from datetime import datetime as dt"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "#LEER DATA TRENDS\r\n",
    "df=pd.read_csv(r\"C:\\Users\\bustossanchez.9\\OneDrive - Teleperformance\\Procesos\\Instacard\\RawData\\Data_TM.csv\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "############### LIMPIEZA DE TEXTO EN FEEDBACK ##########################\r\n",
    "#Lowercase: \r\n",
    "df[\"Feedback\"] = df[\"Feedback\"].apply(lambda x: str(x).lower())\r\n",
    "\r\n",
    "#Se aplica la funcion a la columna\r\n",
    "df['Feedback'] = df['Feedback'].apply(lambda x: x.encode('utf-8').decode('ascii', 'ignore'))\r\n",
    "\r\n",
    "\r\n",
    "#Remover puntuacion\r\n",
    "df['Feedback'] = df['Feedback'].apply(lambda x: x.replace(\".\", \"\"))\r\n",
    "df['Feedback'] = df['Feedback'].apply(lambda x: x.replace(\",\", \"\"))\r\n",
    "df['Feedback'] = df['Feedback'].apply(lambda x: x.replace(\"!\", \"\"))\r\n",
    "df=df.set_index(\"Date\")\r\n",
    "\r\n",
    "# LISTA Numero de palabras:\r\n",
    "df[\"words\"] = df[\"Feedback\"].apply(lambda x: list(nltk.word_tokenize(x)))\r\n",
    "\r\n",
    "#Remover stopwords y puntuacion\r\n",
    "def remove_stopwords(listica):\r\n",
    "    from nltk.corpus import stopwords                   \r\n",
    "    stop_words = set(stopwords.words(\"english\"))        #Getting a list of stopwords\r\n",
    "    removedstopwords = [w for w in listica if not w in stop_words]  # code to remove stop words\r\n",
    "    return removedstopwords\r\n",
    "\r\n",
    "df[\"words\"] = df[\"words\"].apply(lambda x: remove_stopwords(x))\r\n",
    "\r\n",
    "#Lemmatizacion: \r\n",
    "def lemmatizacion(listica2):\r\n",
    "    WNlemma = nltk.WordNetLemmatizer()       #Getting a list of stopwords\r\n",
    "    lemmatized_output = [WNlemma.lemmatize(w, pos = \"v\") for w in listica2]  # code to remove stop words\r\n",
    "    return lemmatized_output\r\n",
    "\r\n",
    "df[\"lemma\"]=df[\"words\"].apply(lambda x: lemmatizacion(x))\r\n",
    "\r\n",
    "df=df.drop([\"words\"], axis=1)\r\n",
    "def common_bigramss(listica3):\r\n",
    "    #lista_nueva = listica3.split(',') # Volvemos el string una lista cuyos valores estan separados por ','\r\n",
    "    md_bigrams = list(nltk.bigrams(listica3))\r\n",
    "    #threshold = 2\r\n",
    "    #filtered_bigrams = [bigram for bigram in md_bigrams if len(bigram[0])>threshold and len(bigram[1])>threshold]\r\n",
    "    #filtered_bigram_dist = FreqDist(filtered_bigrams)\r\n",
    "    return md_bigrams\r\n",
    "df[\"bigrams\"]=df[\"lemma\"].apply(lambda x: common_bigramss(x))\r\n",
    "df=df.drop(\"lemma\", axis=1)\r\n",
    "df[\"bigrams\"]=df[\"bigrams\"].map(str)\r\n",
    "df[\"bigrams\"]=df[\"bigrams\"].apply(lambda x: x.replace(\"'\",\"\"))\r\n",
    "df[\"bigrams\"]=df[\"bigrams\"].apply(lambda x: x.replace(\" \",\"\"))\r\n",
    "\r\n",
    "df1=df\r\n",
    "df2=df\r\n",
    "df3=df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "#####CATEGORIZATION AGENT################\r\n",
    "Categories_agent = {\"Contact Issues\": [\"(end,chat)\",\"(leave,chat)\",\"(chat,end)\",\"(chat,without)\",\"(take,time)\",\"(timely,manner)\",\"(time,get)\",\"(put,hold)\",\"(disconnect,chat)\",\"(speak,supervisor)\",\"(close,chat)\",\"(ask,speak)\",\"(agent,end)\",\"(supervisor,call)\",\"(speak,manager)\",\"(let,speak)\",\"(agent,rude)\"],\r\n",
    "                    \"Customer Service\": [\"(helpful,thank)\",\"(great,job)\",\"(thank,help)\",\"(service,rep)\",\"(friendly,helpful)\",\"(good,job)\",\"(service,representative)\",\"(support,person)\",\"(service,agent)\",\"(service,excellent)\",\"(person,help)\",\"(great,experience)\",\"(agent,helpful)\",\"(job,thank)\",\"(support,agent)\",\"(great,work)\",\"(good,work)\",\"(agent,great)\",\"(agent,help)\",\"(support,rep)\",\"(great,agent)\",\"(support,people)\",\"(agent,good)\"],\r\n",
    "                    \"Issue Resolution\":[\"(resolve,issue)\",\"(answer,question)\",\"(issue,resolve)\",\"(solve,problem)\",\"(solve,issue)\",\"(never,receive)\",\"(resolve,problem)\",\"(never,get)\",\"(issue,quickly)\",\"(fix,problem)\",\"(fix,issue)\",\"(go,beyond)\",\"(didnt,get)\",\"(problem,solve)\",\"(quick,response)\",\"(problem,resolve)\",\"(someone,else)\",\"(help,resolve)\",\"(could,get)\",\"(let,know)\",\"(issue,thank)\",\"(get,help)\",\"(still,wait)\",\"(help,issue)\",\"(care,issue)\",\"(could,help)\",\"(able,help)\",\"(get,answer)\",\"(try,get)\",\"(question,answer)\",\"(keep,get)\",\"(try,help)\",\"(resolve,quickly)\",\"(didnt,help)\",\"(issue,still)\",\"(help,get)\",\"(address,issue)\",\"(able,resolve)\",\"(still,receive)\",\"(help,solve)\",\"(get,issue)\",\"(could,find)\",\"(still,resolve)\",\"(handle,issue)\",\"(ask,question)\",\"(need,help)\",\"(quick,easy)\",\"(help,problem)\",\"(get,resolve)\",\"(person,speak)\",\"(get,response)\",\"(help,need)\",\"(nothing,resolve)\",\"(issue,never)\",\"(issue,solve)\",\"(still,get)\",\"(never,resolve)\",\"(resolve,thank)\",\"(issue,take)\",\"(issue,great)\",\"(try,resolve)\",\"(resolution,problem)\",\"(issue,get)\",\"(question,ask)\",\"(receive,response)\",\"(without,help)\",\"(could,resolve)\",\"(really,help)\",\"(help,fix)\",\"(problem,fix)\",\"(couldnt,help)\",\"(time,thank)\",\"(give,answer)\",\"(problem,still)\",\"(problem,get)\",\"(find,solution)\",\"(didnt,resolve)\",\"(get,resolution)\",\"(resolve,situation)\",\"(wasnt,resolve)\",\"(run,around)\",\"(solution,problem)\",\"(get,problem)\",\"(really,need)\",\"(time,issue)\",\"(unable,help)\",\"(without,resolve)\",\"(actually,help)\",\"(solve,problems)\",\"(still,unresolved)\",\"(back,forth)\",\"(still,need)\",\"(explain,issue)\",\"(help,understand)\",\"(help,customer)\",\"(help,find)\",\"(issue,fix)\",\"(issue,unresolved)\",\"(answer,phone)\",\"(resolve,concern)\",\"(give,information)\",\"(resolve,still)\",\"(unable,resolve)\",\"(agent,say)\",\"(information,need)\",\"(know,answer)\",\"(issue,give)\",\"(issue,could)\",\"(agent,tell)\",\"(need,fix)\",\"(problem,never)\",\"(issue,need)\",\"(issue,agent)\",\"(issue,please)\",\"(resolve,anything)\",\"(issue,first)\",\"(get,fix)\",\"(answer,give)\",\"(issue,support)\",\"(resolve,didnt)\",\"(agent,could)\",\"(remain,unresolved)\",\"(issue,dont)\",\"(resolve,question)\",\"(provide,information)\",\"(still,issue)\",\"(everything,handle)\",\"(support,issue)\",\"(actually,resolve)\"],\r\n",
    "                    \"English Skills\": [\"(could,understand)\",\"(understand,situation)\",\"(hard,understand)\",\"(get,someone)\",\"(understand,say)\",\"(difficult,understand)\",\"(speak,english)\",\"(hire,people)\",\"(hard,time)\",\"(agent,speak)\",\"(time,understand)\",\"(could,barely)\",\"(try,understand)\",\"(better,communication)\",\"(language,barrier)\",\"(people,speak)\",\"(barely,understand)\",\"(english,speak)\",\"(understand,english)\",\"(english,language)\",\"(barely,speak)\"],\r\n",
    "                    \"Listen/Understand\":[\"(understand,issue)\",\"(understand,problem)\",\"(pay,attention)\",\"(dont,understand)\",\"(didnt,understand)\",\"(listen,issue)\",\"(agent,understand)\",\"(issue,understand)\",\"(listen,customer)\",\"(understand,try)\",\"(couldnt,understand)\"]\r\n",
    "                    }\r\n",
    "\r\n",
    "################### FUNCION PARA CATEGORIZAR ############################################\r\n",
    "def get_category(x):\r\n",
    "    key_list = []\r\n",
    "    for key in Categories_agent.keys():\r\n",
    "        for value in Categories_agent[key]:\r\n",
    "            if value in x:\r\n",
    "                if key not in key_list:\r\n",
    "                    key_list.append(key)\r\n",
    "    return(key_list)\r\n",
    "\r\n",
    "################### FUNCION PARA RETORNAR EL BIGRAMA ##################################3\r\n",
    "\r\n",
    "def get_bigram_key(x):\r\n",
    "    key_list = []\r\n",
    "    for value in Categories_agent[x[\"Category\"]]:\r\n",
    "        if value in x[\"bigrams\"]:\r\n",
    "            if value not in key_list:\r\n",
    "                key_list.append(value)\r\n",
    "    return(key_list)\r\n",
    "\r\n",
    "# Se aplica la funcion al df:\r\n",
    "df1[\"Category\"]=df1[\"bigrams\"].apply(lambda x: get_category(x))\r\n",
    "df1[\"Category\"]=df1[\"Category\"].map(str)\r\n",
    "\r\n",
    "df1=df1[df1[\"Category\"]!=\"[]\"]\r\n",
    "df1[\"Category\"]=df1[\"Category\"].map(str).apply(lambda x: x.replace(\"[\",\"\"))\r\n",
    "df1[\"Category\"]=df1[\"Category\"].map(str).apply(lambda x: x.replace(\"]\",\"\"))\r\n",
    "df1[\"Category\"]=df1[\"Category\"].map(str).apply(lambda x: x.replace(\"'\",\"\"))\r\n",
    "df1[\"Category\"]=df1[\"Category\"].map(str).apply(lambda x: x.strip())\r\n",
    "df1[\"Category\"]=df1[\"Category\"].map(str)\r\n",
    "df1.Category = df1.Category.str.split(\",\")\r\n",
    "df1 = df1.explode('Category')\r\n",
    "df1[\"Category\"]=df1[\"Category\"].map(str).apply(lambda x: x.strip())\r\n",
    "#Obtener bigramas por categoria: \r\n",
    "df1[\"Category2\"]=df1.apply(get_bigram_key, axis=1)\r\n",
    "df1[\"Category2\"]=df1[\"Category2\"].map(str).apply(lambda x: x.replace(\"[\",\"\"))\r\n",
    "df1[\"Category2\"]=df1[\"Category2\"].map(str).apply(lambda x: x.replace(\"]\",\"\"))\r\n",
    "df1[\"Category2\"]=df1[\"Category2\"].map(str).apply(lambda x: x.replace(\"'\",\"\"))\r\n",
    "df1[\"Category2\"]=df1[\"Category2\"].map(str).apply(lambda x: x.replace(\"), (\",\"-\"))\r\n",
    "df1[\"Category2\"]=df1[\"Category2\"].map(str)\r\n",
    "df1.Category2 = df1.Category2.str.split(\"-\")\r\n",
    "df1 = df1.explode('Category2')\r\n",
    "df1[\"Category2\"]=df1[\"Category2\"].map(str).apply(lambda x: x.replace(\"(\",\"\"))\r\n",
    "df1[\"Category2\"]=df1[\"Category2\"].map(str).apply(lambda x: x.replace(\")\",\"\"))\r\n",
    "df1[\"Category2\"]=df1[\"Category2\"].map(str).apply(lambda x: x.replace(\",\",\" \"))\r\n",
    "df1[\"Category2\"]=df1[\"Category2\"].map(str).apply(lambda x: x.strip())\r\n",
    "df1=df1.drop(\"bigrams\", axis=1)\r\n",
    "df1[\"Category1\"]=\"Agent\"\r\n",
    "df1.to_csv(r\"C:\\Users\\bustossanchez.9\\OneDrive - Teleperformance\\Procesos\\Instacard\\RawData\\Categorization_Agent.csv\")\r\n",
    "\r\n",
    "\r\n",
    "#####CATEGORIZATION BRAND ################\r\n",
    "Categories_brand = {\"Account\":[\"(cancel,account)\",\"(email,address)\",\"(close,account)\",\"(account,still)\",\"(account,deactivate)\",\"(account,get)\",\"(account,tell)\",\"(get,account)\",\"(delete,account)\",\"(deactivate,account)\",\"(new,account)\",\"(reset,password)\",\"(account,please)\",\"(account,hack)\",\"(help,account)\",\"(account,issue)\"],\r\n",
    "                    \"App\": [\"(use,app)\",\"(app,say)\",\"(app,work)\",\"(delete,app)\",\"(app,order)\",\"(issue,app)\",\"(app,issue)\",\"(app,need)\",\"(fix,app)\"],\r\n",
    "                    \"Billing/Payment\":[\"(credit,card)\",\"(get,refund)\",\"(credit,account)\",\"(give,credit)\",\"(get,credit)\",\"(receive,refund)\",\"(gift,card)\",\"(money,back)\",\"(bank,account)\",\"(receive,credit)\",\"(debit,card)\",\"(offer,credit)\",\"(full,refund)\",\"(refund,credit)\",\"(refund,money)\",\"(refund,order)\",\"(order,charge)\",\"(want,refund)\",\"(get,charge)\",\"(promo,code)\",\"(get,money)\",\"(charge,card)\",\"(issue,credit)\",\"(charge,twice)\",\"(credit,order)\",\"(issue,refund)\",\"(give,refund)\",\"(back,account)\",\"(take,money)\",\"(need,refund)\",\"(account,credit)\",\"(want,money)\",\"(want,credit)\",\"(credit,back)\",\"(order,refund)\",\"(need,change)\",\"(payment,method)\",\"(charge,account)\",\"(offer,refund)\",\"(pay,order)\",\"(order,credit)\",\"(money,refund)\",\"(pay,extra)\",\"(refund,back)\",\"(get,full)\",\"(need,money)\",\"(back,card)\",\"(credit,refund)\",\"(give,money)\",\"(refund,issue)\",\"(money,account)\",\"(get,cancel)\",\"(steal,money)\",\"(refund,account)\",\"(full,amount)\",\"(time,money)\",\"(issue,account)\",\"(money,order)\",\"(get,pay)\",\"(pay,full)\",\"(refund,cancel)\",\"(need,support)\",\"(refund,give)\",\"(card,decline)\",\"(receive,money)\",\"(refund,full)\"],\r\n",
    "                    \"Contact Issues\": [\"(call,back)\",\"(get,back)\",\"(come,back)\",\"(second,time)\",\"(receive,email)\",\"(send,email)\",\"(call,customer)\",\"(response,time)\",\"(contact,customer)\",\"(contact,support)\",\"(take,minutes)\",\"(hear,back)\",\"(email,say)\",\"(call,support)\",\"(time,take)\",\"(wait,long)\",\"(back,get)\",\"(contact,driver)\",\"(try,contact)\",\"(get,touch)\",\"(amount,time)\",\"(wait,days)\",\"(tell,contact)\",\"(text,message)\",\"(able,contact)\",\"(time,contact)\",\"(time,resolve)\",\"(way,contact)\",\"(never,call)\",\"(order,contact)\",\"(need,call)\",\"(get,contact)\",\"(answer,call)\",\"(need,contact)\",\"(someone,contact)\",\"(contact,someone)\"],\r\n",
    "                    \"Customer Service\":[\"(great,customer)\",\"(great,service)\",\"(excellent,customer)\",\"(service,thank)\",\"(excellent,service)\",\"(customer,support)\",\"(great,help)\",\"(super,helpful)\",\"(good,customer)\",\"(great,support)\",\"(excellent,support)\",\"(bad,experience)\",\"(several,time)\",\"(terrible,service)\",\"(terrible,customer)\",\"(horrible,customer)\",\"(support,thank)\",\"(best,customer)\",\"(good,support)\",\"(worst,customer)\",\"(horrible,experience)\",\"(poor,service)\",\"(thank,support)\",\"(support,team)\",\"(different,people)\",\"(bad,service)\",\"(awesome,thank)\",\"(keep,good)\",\"(need,better)\",\"(support,staff)\",\"(service,suck)\",\"(tech,support)\",\"(support,always)\",\"(support,get)\",\"(care,customers)\",\"(better,customer)\",\"(better,service)\",\"(reach,support)\"],\r\n",
    "                    \"Instacart\":[\"(use,instacart)\",\"(use,service)\",\"(never,use)\"],\r\n",
    "                    \"Orders\":[\"(place,order)\",\"(cancel,order)\",\"(get,order)\",\"(receive,order)\",\"(wrong,order)\",\"(get,groceries)\",\"(order,cancel)\",\"(order,never)\",\"(miss,items)\",\"(order,place)\",\"(order,get)\",\"(items,order)\",\"(miss,order)\",\"(order,receive)\",\"(order,wrong)\",\"(issue,order)\",\"(entire,order)\",\"(pick,order)\",\"(say,order)\",\"(order,miss)\",\"(order,arrive)\",\"(original,order)\",\"(find,order)\",\"(order,time)\",\"(order,items)\",\"(make,order)\",\"(order,take)\",\"(order,say)\",\"(complete,order)\",\"(help,order)\",\"(wait,order)\",\"(order,leave)\",\"(order,come)\",\"(problem,order)\",\"(another,order)\",\"(put,order)\",\"(correct,order)\",\"(take,order)\",\"(leave,order)\",\"(charge,order)\",\"(want,order)\",\"(item,order)\",\"(order,make)\",\"(order,late)\",\"(order,customer)\",\"(order,pick)\",\"(never,order)\",\"(replace,order)\",\"(try,order)\",\"(whole,order)\",\"(order,wasnt)\",\"(order,complete)\",\"(order,send)\",\"(order,ready)\",\"(give,wrong)\",\"(order,correct)\",\"(order,pay)\",\"(see,order)\",\"(order,great)\",\"(order,another)\",\"(change,order)\",\"(order,without)\",\"(order,since)\",\"(back,order)\",\"(call,order)\",\"(account,order)\",\"(miles,away)\",\"(could,order)\",\"(add,order)\",\"(elses,order)\",\"(half,order)\",\"(bring,order)\",\"(right,order)\",\"(assign,order)\",\"(order,right)\",\"(order,next)\",\"(drop,order)\",\"(give,order)\",\"(order,already)\",\"(order,offer)\",\"(late,order)\",\"(order,drop)\",\"(order,cant)\",\"(send,order)\",\"(ask,order)\",\"(order,process)\",\"(order,spend)\",\"(order,check)\",\"(order,system)\",\"(fix,order)\",\"(order,completely)\",\"(multiple,order)\",\"(order,lose)\",\"(accept,order)\",\"(order,driver)\",\"(order,resolve)\",\"(customer,order)\",\"(confirm,order)\",\"(incorrect,order)\",\"(process,order)\",\"(order,change)\",\"(order,nothing)\",\"(twice,order)\",\"(order,confirm)\",\"(order,mess)\",\"(start,order)\",\"(cant,order)\",\"(order,incorrect)\",\"(let,order)\",\"(mess,order)\",\"(customers,order)\",\"(mark,order)\",\"(full,order)\"],\r\n",
    "                    \"Orders-Delivery\":[\"(order,deliver)\",\"(deliver,wrong)\",\"(wrong,address)\",\"(delivery,time)\",\"(deliver,order)\",\"(wrong,house)\",\"(free,delivery)\",\"(delivery,fee)\",\"(never,deliver)\",\"(get,delivery)\",\"(order,delivery)\",\"(items,deliver)\",\"(delivery,service)\",\"(delivery,order)\",\"(get,deliver)\",\"(correct,address)\",\"(delivery,driver)\",\"(suppose,deliver)\",\"(say,deliver)\",\"(time,delivery)\",\"(change,delivery)\",\"(deliver,items)\",\"(food,deliver)\",\"(deliver,time)\",\"(hour,delivery)\",\"(charge,delivery)\",\"(deliver,correct)\",\"(deliver,food)\",\"(delivery,charge)\",\"(could,deliver)\",\"(deliver,right)\",\"(delivery,address)\",\"(delivery,guy)\",\"(pay,delivery)\",\"(deliver,address)\",\"(issue,delivery)\",\"(right,address)\",\"(first,delivery)\",\"(go,deliver)\",\"(delivery,instructions)\",\"(delivery,make)\",\"(delivery,issue)\",\"(delivery,get)\",\"(still,deliver)\",\"(delivery,deliver)\",\"(deliver,get)\",\"(late,delivery)\",\"(actually,deliver)\",\"(deliver,customer)\",\"(refund,delivery)\",\"(food,delivery)\",\"(address,order)\",\"(time,deliver)\",\"(pick,delivery)\",\"(problem,address)\",\"(address,delivery)\",\"(deliver,deliver)\",\"(delivery,cancel)\",\"(delivery,experience)\",\"(another,address)\",\"(delivery,take)\",\"(didnt,deliver)\",\"(address,wrong)\"],\r\n",
    "                    \"Orders-Store\":[\"(miss,item)\",\"(store,close)\",\"(order,store)\",\"(store,order)\",\"(fulfill,order)\",\"(part,order)\"],\r\n",
    "                    \"Orders-Shopper\":[\"(delivery,person)\",\"(driver,deliver)\",\"(follow,instructions)\",\"(person,deliver)\",\"(delivery,people)\",\"(check,order)\",\"(another,delivery)\",\"(follow,directions)\",\"(delivery,drivers)\",\"(get,better)\",\"(sure,order)\"],\r\n",
    "                    \"Time\":[\"(take,long)\",\"(waste,time)\",\"(long,time)\",\"(time,order)\",\"(order,delay)\",\"(two,hours)\",\"(hours,later)\",\"(wait,hours)\",\"(multiple,time)\",\"(within,hours)\",\"(wait,minutes)\",\"(hours,late)\",\"(many,time)\",\"(wait,time)\",\"(order,hours)\",\"(business,days)\",\"(take,hours)\",\"(half,hour)\",\"(hold,minutes)\",\"(much,time)\",\"(long,wait)\",\"(hours,order)\",\"(days,ago)\",\"(days,get)\",\"(minutes,later)\",\"(order,wait)\",\"(take,hour)\",\"(hours,get)\",\"(almost,hours)\",\"(hour,half)\",\"(time,wait)\",\"(time,give)\",\"(pick,time)\",\"(minutes,get)\",\"(hour,later)\",\"(time,receive)\",\"(time,spend)\",\"(time,waste)\",\"(almost,hour)\",\"(hour,time)\",\"(two,weeks)\",\"(hour,late)\",\"(one,hour)\",\"(hour,get)\",\"(waste,hour)\",\"(wait,hour)\",\"(wait,min)\",\"(minutes,wait)\",\"(order,hour)\",\"(wait,almost)\",\"(hour,wait)\",\"(faster,response)\"],\r\n",
    "                    \"Zendesk\":[\"(service,zendesk)\",\"(helpful,zendesk)\",\"(thank,zendesk)\",\"(help,zendesk)\",\"(zendesk,great)\",\"(zendesk,excellent)\",\"(issue,zendesk)\",\"(zendesk,helpful)\",\"(zendesk,issue)\",\"(zendesk,support)\",\"(resolve,zendesk)\",\"(chat,zendesk)\",\"(zendesk,resolution)\"]\r\n",
    "                    }\r\n",
    "\r\n",
    "################### FUNCION PARA CATEGORIZAR ############################################\r\n",
    "def get_category(x):\r\n",
    "    key_list = []\r\n",
    "    for key in Categories_brand.keys():\r\n",
    "        for value in Categories_brand[key]:\r\n",
    "            if value in x:\r\n",
    "                if key not in key_list:\r\n",
    "                    key_list.append(key)\r\n",
    "    return(key_list)\r\n",
    "\r\n",
    "################### FUNCION PARA RETORNAR EL BIGRAMA ##################################3\r\n",
    "\r\n",
    "def get_bigram_key(x):\r\n",
    "    key_list = []\r\n",
    "    for value in Categories_brand[x[\"Category\"]]:\r\n",
    "        if value in x[\"bigrams\"]:\r\n",
    "            if value not in key_list:\r\n",
    "                key_list.append(value)\r\n",
    "    return(key_list)\r\n",
    "\r\n",
    "# Se aplica la funcion al df:\r\n",
    "df2[\"Category\"]=df2[\"bigrams\"].apply(lambda x: get_category(x))\r\n",
    "df2[\"Category\"]=df2[\"Category\"].map(str)\r\n",
    "\r\n",
    "df2=df2[df2[\"Category\"]!=\"[]\"]\r\n",
    "df2[\"Category\"]=df2[\"Category\"].map(str).apply(lambda x: x.replace(\"[\",\"\"))\r\n",
    "df2[\"Category\"]=df2[\"Category\"].map(str).apply(lambda x: x.replace(\"]\",\"\"))\r\n",
    "df2[\"Category\"]=df2[\"Category\"].map(str).apply(lambda x: x.replace(\"'\",\"\"))\r\n",
    "df2[\"Category\"]=df2[\"Category\"].map(str).apply(lambda x: x.strip())\r\n",
    "df2[\"Category\"]=df2[\"Category\"].map(str)\r\n",
    "df2.Category = df2.Category.str.split(\",\")\r\n",
    "df2 = df2.explode('Category')\r\n",
    "df2[\"Category\"]=df2[\"Category\"].map(str).apply(lambda x: x.strip())\r\n",
    "#Obtener bigramas por categoria: \r\n",
    "df2[\"Category2\"]=df2.apply(get_bigram_key, axis=1)\r\n",
    "df2[\"Category2\"]=df2[\"Category2\"].map(str).apply(lambda x: x.replace(\"[\",\"\"))\r\n",
    "df2[\"Category2\"]=df2[\"Category2\"].map(str).apply(lambda x: x.replace(\"]\",\"\"))\r\n",
    "df2[\"Category2\"]=df2[\"Category2\"].map(str).apply(lambda x: x.replace(\"'\",\"\"))\r\n",
    "df2[\"Category2\"]=df2[\"Category2\"].map(str).apply(lambda x: x.replace(\"), (\",\"-\"))\r\n",
    "df2[\"Category2\"]=df2[\"Category2\"].map(str)\r\n",
    "df2.Category2 = df2.Category2.str.split(\"-\")\r\n",
    "df2 = df2.explode('Category2')\r\n",
    "df2[\"Category2\"]=df2[\"Category2\"].map(str).apply(lambda x: x.replace(\"(\",\"\"))\r\n",
    "df2[\"Category2\"]=df2[\"Category2\"].map(str).apply(lambda x: x.replace(\")\",\"\"))\r\n",
    "df2[\"Category2\"]=df2[\"Category2\"].map(str).apply(lambda x: x.replace(\",\",\" \"))\r\n",
    "df2[\"Category2\"]=df2[\"Category2\"].map(str).apply(lambda x: x.strip())\r\n",
    "df2=df2.drop(\"bigrams\", axis=1)\r\n",
    "df2[\"Category1\"]=\"Brand\"\r\n",
    "df2.to_csv(r\"C:\\Users\\bustossanchez.9\\OneDrive - Teleperformance\\Procesos\\Instacard\\RawData\\Categorization_Brand.csv\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "#####CATEGORIZATION CHANNEL ################\r\n",
    "Categories_channel = {\"Phone\":[\"(phone,number)\",\"(phone,call)\",\"(speak,someone)\",\"(get,phone)\",\"(receive,call)\",\"(someone,call)\",\"(person,talk)\",\"(phone,support)\",\"(talk,someone)\",\"(pick,phone)\",\"(get,call)\",\"(hang,phone)\",\"(need,speak)\",\"(one,call)\",\"(time,phone)\",\"(support,phone)\",\"(please,call)\",\"(call,chat)\",\"(live,person)\",\"(never,hear)\"],\r\n",
    "                        \"Chat\":[\"(chat,support)\",\"(chat,get)\",\"(online,chat)\",\"(time,chat)\"],\r\n",
    "                        \"Email\":[\"(get,email)\",\"(via,email)\",\"(email,send)\"]}\r\n",
    "\r\n",
    "################### FUNCION PARA CATEGORIZAR ############################################\r\n",
    "def get_category(x):\r\n",
    "    key_list = []\r\n",
    "    for key in Categories_channel.keys():\r\n",
    "        for value in Categories_channel[key]:\r\n",
    "            if value in x:\r\n",
    "                if key not in key_list:\r\n",
    "                    key_list.append(key)\r\n",
    "    return(key_list)\r\n",
    "\r\n",
    "################### FUNCION PARA RETORNAR EL BIGRAMA ##################################3\r\n",
    "\r\n",
    "def get_bigram_key(x):\r\n",
    "    key_list = []\r\n",
    "    for value in Categories_channel[x[\"Category\"]]:\r\n",
    "        if value in x[\"bigrams\"]:\r\n",
    "            if value not in key_list:\r\n",
    "                key_list.append(value)\r\n",
    "    return(key_list)\r\n",
    "\r\n",
    "# Se aplica la funcion al df:\r\n",
    "df3[\"Category\"]=df3[\"bigrams\"].apply(lambda x: get_category(x))\r\n",
    "df3[\"Category\"]=df3[\"Category\"].map(str)\r\n",
    "\r\n",
    "df3=df3[df3[\"Category\"]!=\"[]\"]\r\n",
    "df3[\"Category\"]=df3[\"Category\"].map(str).apply(lambda x: x.replace(\"[\",\"\"))\r\n",
    "df3[\"Category\"]=df3[\"Category\"].map(str).apply(lambda x: x.replace(\"]\",\"\"))\r\n",
    "df3[\"Category\"]=df3[\"Category\"].map(str).apply(lambda x: x.replace(\"'\",\"\"))\r\n",
    "df3[\"Category\"]=df3[\"Category\"].map(str).apply(lambda x: x.strip())\r\n",
    "df3[\"Category\"]=df3[\"Category\"].map(str)\r\n",
    "df3.Category = df3.Category.str.split(\",\")\r\n",
    "df3 = df3.explode('Category')\r\n",
    "df3[\"Category\"]=df3[\"Category\"].map(str).apply(lambda x: x.strip())\r\n",
    "#Obtener bigramas por categoria: \r\n",
    "df3[\"Category2\"]=df3.apply(get_bigram_key, axis=1)\r\n",
    "df3[\"Category2\"]=df3[\"Category2\"].map(str).apply(lambda x: x.replace(\"[\",\"\"))\r\n",
    "df3[\"Category2\"]=df3[\"Category2\"].map(str).apply(lambda x: x.replace(\"]\",\"\"))\r\n",
    "df3[\"Category2\"]=df3[\"Category2\"].map(str).apply(lambda x: x.replace(\"'\",\"\"))\r\n",
    "df3[\"Category2\"]=df3[\"Category2\"].map(str).apply(lambda x: x.replace(\"), (\",\"-\"))\r\n",
    "df3[\"Category2\"]=df3[\"Category2\"].map(str)\r\n",
    "df3.Category2 = df3.Category2.str.split(\"-\")\r\n",
    "df3 = df3.explode('Category2')\r\n",
    "df3[\"Category2\"]=df3[\"Category2\"].map(str).apply(lambda x: x.replace(\"(\",\"\"))\r\n",
    "df3[\"Category2\"]=df3[\"Category2\"].map(str).apply(lambda x: x.replace(\")\",\"\"))\r\n",
    "df3[\"Category2\"]=df3[\"Category2\"].map(str).apply(lambda x: x.replace(\",\",\" \"))\r\n",
    "df3[\"Category2\"]=df3[\"Category2\"].map(str).apply(lambda x: x.strip())\r\n",
    "df3=df3.drop(\"bigrams\", axis=1)\r\n",
    "df3[\"Category1\"]=\"Channel\"\r\n",
    "df3.to_csv(r\"C:\\Users\\bustossanchez.9\\OneDrive - Teleperformance\\Procesos\\Instacard\\RawData\\Categorization_Channel.csv\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "####CONCATENADOR###############\r\n",
    "dfa=pd.read_csv(r\"C:\\Users\\bustossanchez.9\\OneDrive - Teleperformance\\Procesos\\Instacard\\RawData\\Categorization_Agent.csv\")\r\n",
    "dfb=pd.read_csv(r\"C:\\Users\\bustossanchez.9\\OneDrive - Teleperformance\\Procesos\\Instacard\\RawData\\Categorization_Brand.csv\")\r\n",
    "dfc=pd.read_csv(r\"C:\\Users\\bustossanchez.9\\OneDrive - Teleperformance\\Procesos\\Instacard\\RawData\\Categorization_Channel.csv\")\r\n",
    "\r\n",
    "conc=[dfa,dfb,dfc]\r\n",
    "\r\n",
    "df_final=pd.concat(conc)\r\n",
    "df_final=df_final.set_index(\"Date\")\r\n",
    "\r\n",
    "pol= lambda x: TextBlob(x).sentiment.polarity\r\n",
    "\r\n",
    "df_final[\"Polarity\"]=df_final['Feedback'].apply(pol)\r\n",
    "\r\n",
    "df_final.head()\r\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Agent</th>\n",
       "      <th>EMAIL</th>\n",
       "      <th>Type</th>\n",
       "      <th>Channel</th>\n",
       "      <th>CR</th>\n",
       "      <th>Menu_Path</th>\n",
       "      <th>CSAT</th>\n",
       "      <th>Feedback</th>\n",
       "      <th>TOUCH</th>\n",
       "      <th>WAREHOUSE</th>\n",
       "      <th>Category</th>\n",
       "      <th>Category2</th>\n",
       "      <th>Category1</th>\n",
       "      <th>Polarity</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-06-21</th>\n",
       "      <td>Silvia Ojeda Vizcarra</td>\n",
       "      <td>silvia.ojedavizcarra@teleperformance.com</td>\n",
       "      <td>Customers</td>\n",
       "      <td>Chat</td>\n",
       "      <td>Entire Delivery Missing</td>\n",
       "      <td>FDO</td>\n",
       "      <td>Bad</td>\n",
       "      <td>have representatives who can quickly identify ...</td>\n",
       "      <td>2</td>\n",
       "      <td>Kroger Delivery Now</td>\n",
       "      <td>Issue Resolution</td>\n",
       "      <td>resolve issue</td>\n",
       "      <td>Agent</td>\n",
       "      <td>0.016667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-06-21</th>\n",
       "      <td>Silvia Ojeda Vizcarra</td>\n",
       "      <td>silvia.ojedavizcarra@teleperformance.com</td>\n",
       "      <td>Customers</td>\n",
       "      <td>Chat</td>\n",
       "      <td>Entire Delivery Missing</td>\n",
       "      <td>FDO</td>\n",
       "      <td>Bad</td>\n",
       "      <td>have representatives who can quickly identify ...</td>\n",
       "      <td>2</td>\n",
       "      <td>Kroger Delivery Now</td>\n",
       "      <td>Issue Resolution</td>\n",
       "      <td>issue take</td>\n",
       "      <td>Agent</td>\n",
       "      <td>0.016667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-06-21</th>\n",
       "      <td>Harumi Rocio Villanueva Navarro</td>\n",
       "      <td>harumi.villanuevanavarro@teleperformance.com</td>\n",
       "      <td>Customers</td>\n",
       "      <td>Phone</td>\n",
       "      <td>Missing Items</td>\n",
       "      <td>T1</td>\n",
       "      <td>Bad</td>\n",
       "      <td>still waiting to receive an email receipt for ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Sprouts Farmers Market</td>\n",
       "      <td>Issue Resolution</td>\n",
       "      <td>still wait</td>\n",
       "      <td>Agent</td>\n",
       "      <td>-0.133333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-06-21</th>\n",
       "      <td>Carla Soller</td>\n",
       "      <td>carla.sollersedano@teleperformance.com</td>\n",
       "      <td>Customers</td>\n",
       "      <td>Phone</td>\n",
       "      <td>Missing Items</td>\n",
       "      <td>T1</td>\n",
       "      <td>Good</td>\n",
       "      <td>instacart has the very best customer service  ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Food Lion</td>\n",
       "      <td>Issue Resolution</td>\n",
       "      <td>problem solve</td>\n",
       "      <td>Agent</td>\n",
       "      <td>0.529167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-06-21</th>\n",
       "      <td>Edith Cielo Changa Rojas</td>\n",
       "      <td>edith.changarojas@teleperformance.co</td>\n",
       "      <td>Customers</td>\n",
       "      <td>Chat</td>\n",
       "      <td>Wrong Order</td>\n",
       "      <td>FDO</td>\n",
       "      <td>Bad</td>\n",
       "      <td>i didnt get to respond</td>\n",
       "      <td>1</td>\n",
       "      <td>Market Basket</td>\n",
       "      <td>Issue Resolution</td>\n",
       "      <td>didnt get</td>\n",
       "      <td>Agent</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Agent  \\\n",
       "Date                                          \n",
       "2023-06-21            Silvia Ojeda Vizcarra   \n",
       "2023-06-21            Silvia Ojeda Vizcarra   \n",
       "2023-06-21  Harumi Rocio Villanueva Navarro   \n",
       "2023-06-21                     Carla Soller   \n",
       "2023-06-21         Edith Cielo Changa Rojas   \n",
       "\n",
       "                                                   EMAIL       Type Channel  \\\n",
       "Date                                                                          \n",
       "2023-06-21      silvia.ojedavizcarra@teleperformance.com  Customers    Chat   \n",
       "2023-06-21      silvia.ojedavizcarra@teleperformance.com  Customers    Chat   \n",
       "2023-06-21  harumi.villanuevanavarro@teleperformance.com  Customers   Phone   \n",
       "2023-06-21        carla.sollersedano@teleperformance.com  Customers   Phone   \n",
       "2023-06-21          edith.changarojas@teleperformance.co  Customers    Chat   \n",
       "\n",
       "                                 CR Menu_Path  CSAT  \\\n",
       "Date                                                  \n",
       "2023-06-21  Entire Delivery Missing       FDO   Bad   \n",
       "2023-06-21  Entire Delivery Missing       FDO   Bad   \n",
       "2023-06-21            Missing Items        T1   Bad   \n",
       "2023-06-21            Missing Items        T1  Good   \n",
       "2023-06-21              Wrong Order       FDO   Bad   \n",
       "\n",
       "                                                     Feedback  TOUCH  \\\n",
       "Date                                                                   \n",
       "2023-06-21  have representatives who can quickly identify ...      2   \n",
       "2023-06-21  have representatives who can quickly identify ...      2   \n",
       "2023-06-21  still waiting to receive an email receipt for ...      1   \n",
       "2023-06-21  instacart has the very best customer service  ...      1   \n",
       "2023-06-21                             i didnt get to respond      1   \n",
       "\n",
       "                         WAREHOUSE          Category      Category2 Category1  \\\n",
       "Date                                                                            \n",
       "2023-06-21     Kroger Delivery Now  Issue Resolution  resolve issue     Agent   \n",
       "2023-06-21     Kroger Delivery Now  Issue Resolution     issue take     Agent   \n",
       "2023-06-21  Sprouts Farmers Market  Issue Resolution     still wait     Agent   \n",
       "2023-06-21               Food Lion  Issue Resolution  problem solve     Agent   \n",
       "2023-06-21           Market Basket  Issue Resolution      didnt get     Agent   \n",
       "\n",
       "            Polarity  \n",
       "Date                  \n",
       "2023-06-21  0.016667  \n",
       "2023-06-21  0.016667  \n",
       "2023-06-21 -0.133333  \n",
       "2023-06-21  0.529167  \n",
       "2023-06-21  0.000000  "
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "df_final.to_csv(r\"C:\\Users\\bustossanchez.9\\OneDrive - Teleperformance\\Procesos\\Instacard\\DataPBI\\Categorization.csv\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "#from datetime import datetime as dt\r\n",
    "print(\"done\", dt.now()) "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "done 2023-06-22 15:34:44.653749\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "#import pymssql\r\n",
    "#import pyodbc\r\n",
    "#import pandas as pd\r\n",
    "#\r\n",
    "#conn = pyodbc.connect('Driver={ODBC Driver 17 for SQL Server};'\r\n",
    "#                     'Server= TPCCP-DB07\\SCOFFS;'\r\n",
    "#                     'Database=Bancolombia;'\r\n",
    "#                     'Trusted_Connection=yes;') \r\n",
    "#\r\n",
    "#data ='''SELECT * FROM [Bancolombia].[dbo].[tbNequiReiteratividad] WITH(NOLOCK)'''\r\n",
    "#\r\n",
    "#BDInfinite=pd.read_sql(sql=data,con=conn)\r\n",
    "#df=BDInfinite\r\n",
    "#\r\n",
    "#df"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.3 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}